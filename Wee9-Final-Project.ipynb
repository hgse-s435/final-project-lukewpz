{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**: describe your dataset, and why you're interested in it\n",
    "\n",
    "The dataset I will use for this study is a list of titles from a self-media company in China. More than ten thousand titles were retrieved online as well as the draft categories of these titles. \n",
    "I am interested in it because this company is the largest self-media source in China, mainly publishing pregnancy and early child-rearing short articles. Thus, through clustering these titles, I want to explore the most discussed topics in this area and validate the draft categories designed by the company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Research question(s)**: describe the overall research question of your project\n",
    "\n",
    "What are the most discussed topics about pregnancy and early child-rearing through clustering the titles of article?\n",
    "\n",
    "Can the clusters of titles validate the categorizations of the topics designed by the self-media company?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypotheses**:\n",
    "    * Describe 2-3 hypotheses that you're planning to test with your dataset\n",
    "    * Each hypoteses should be based on academic research (cite a paper) and/or background knowledge that you have about the dataset if you've collected it yourself (e.g., if you've conducted interviews)\n",
    "    * Each hypotheses should be formulated as an affirmation (and not a question)\n",
    "    * You can also describe alternative hypotheses, if you think that your results could go either way (but again, have a rationale as for why)\n",
    "    \n",
    "    The clusters can reflect the most discussed topics.\n",
    "    The clusters can validate the categorization, but more data is needed, such as the content of each article, in order to accurately validate the categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "    * how are you planning to test each hypothesis? What models are you thinking of using?\n",
    "    * what are the best results you can hope for? Is that interesting / relevant for other researchers?\n",
    "    * what are implications of your potential findings for practioners?\n",
    "    \n",
    "    K-means and DBScan will be the primary methods I will use in this study.\n",
    "    The best results may be the correct validation of the categorization. More interestingly, if the clusters can reflect the frequency of each category, I can conclude that which category occurs the most.\n",
    "    Consequently, the frequency of each category can be reported to the company so that they can write more articles that both interest parents and did not mention frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Threads**\n",
    "    * Describe issues that might arise during the analyses above\n",
    "    * Come up with backup plans in case you run into theses issues\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your raw data below; provide definition / explanations for the measures you're using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean you data in this section, and make sure it's ready to be analyzed for next week!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of all functions\n",
    "\n",
    "#Import data\n",
    "documents = import_data(Content.txt)\n",
    "\n",
    "# Create a dataframe that is exact same as the txt\n",
    "df = create_dataframe(documents)\n",
    "\n",
    "# Clean the dataframe and create a list of titles ready to be used\n",
    "wordlists = clean_data_create_wordlists(df)\n",
    "\n",
    "# Create a seg_list with all words, a vocabulary list, and a dataframe with vocabulary and frequency of ocurrance\n",
    "seg_list, vocabulary, vocab_freq_df = create_seg_list_and_vocabulary(wordlists)\n",
    "\n",
    "# Create 100-word chunks\n",
    "chunks = flatten_and_overlap(seg_list)\n",
    "\n",
    "# Returns a dataframe with the counts of words\n",
    "vector_df = docs_by_words_df(chunks, vocabulary)\n",
    "\n",
    "# Create a function that adds one to the current cell and takes its log if the value in the cell is not zero\n",
    "df_log = vector_df.applymap(one_plus_log)\n",
    "\n",
    "# Applied the normalization\n",
    "df_log = vector_normalizer(df_log)\n",
    "\n",
    "# Compute deviation vectors of each row\n",
    "vector_df = transform_deviation_vectors(df_log)\n",
    "\n",
    "# Apply agglomerative clustering here \n",
    "ward, clf = apply_agglomertive(vector_df, 10)\n",
    "\n",
    "# Print the top 10 words for each cluster centroid\n",
    "top_words = topwords_clusters(vector_df, clf.centroids_.shape[0], clf.centroids_)\n",
    "\n",
    "# Create a after-clustered dataframe\n",
    "master_df = create_clustering_df(vector_df, chunks, ward, wordlists, df)\n",
    "\n",
    "# Create articles count dataframe for data visulization\n",
    "countid = numarticle_count_df(master_df)\n",
    "\n",
    "# Data Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import jieba   #Chinese word segmentation tool\n",
    "import pandas as pd\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "def import_data(filename):\n",
    "\n",
    "    # Import data\n",
    "    with codecs.open('./'+str(filename), 'rb', encoding = 'utf-16') as f:\n",
    "        documents = f.read()\n",
    "\n",
    "    # Delete the column name; replace all segementations to \\t\n",
    "    documents = documents[36:]\n",
    "    documents = [doc.replace('\\n', '') for doc in documents]\n",
    "    documents = [doc.replace(' \"', '') for doc in documents]\n",
    "    documents = [doc.replace('\"', '') for doc in documents]\n",
    "    documents = [doc.replace('\\r', '\\t') for doc in documents]\n",
    "    documents = ''.join(documents)\n",
    "    documents = documents.split('\\t')\n",
    "\n",
    "    return documents\n",
    "\n",
    "documents = import_data(Content.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that is exact same as the txt\n",
    "def create_dataframe(documents):\n",
    "\n",
    "    # Create an empty dataframe\n",
    "    index = range(0,12043)\n",
    "    df = pd.DataFrame(index = index, columns=['一级分类', '二级分类','三级分类','四级分类','五级分类','知识ID','知识标题'])\n",
    "\n",
    "    # Replace each cell of the dataframe with elements in documents\n",
    "    a=0\n",
    "    b=0\n",
    "    for i, word in enumerate(documents):    \n",
    "        if documents[i] == '孕期' or documents[i] == '全龄' or documents[i] == '原创栏目' or documents[i] == '备孕' or documents[i] == '育儿':\n",
    "            b=0\n",
    "            a+=1\n",
    "            df.iloc[a,b] = documents[i]\n",
    "        else:\n",
    "            b+=1\n",
    "            df.iloc[a,b] = documents[i]\n",
    "   \n",
    "    return df\n",
    "\n",
    "df = create_dataframe(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframe and create a list of titles ready to be used\n",
    "def clean_data_create_wordlists(df):\n",
    "\n",
    "    # Delete the row without data\n",
    "    df = df.drop(df.index[0])\n",
    "    df = df.drop(df.index[12043:])\n",
    "    df.head()\n",
    "    df.tail()\n",
    "\n",
    "    # Create a word list of all titles\n",
    "    wordlists = []\n",
    "    for i in df.index:\n",
    "        title = df.loc[i, '知识标题']\n",
    "        wordlists.append(title)\n",
    "\n",
    "    # Clean punctuation\n",
    "    punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "                   '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "                   '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "                   '_', '^', '`', '{', '}', '|', '~', '−', \n",
    "                   '，', '。', '！', '“', '”', '￥', \"‘\" , \"’\", '（',\n",
    "                   '）', '；', '：', '——', '《', '》', '？', '【',\n",
    "                   '】', '—', '……', '「', '」', '、', '＆', '＋',\n",
    "                   '－', '５', '＝', '＞',  'Ｎ', '｜', '～']\n",
    "\n",
    "    with codecs.open('chinese_stopwords_list_text.txt', 'rb', encoding = 'utf-8') as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords = stopwords[1:]\n",
    "    stopwords = stopwords.split('\\n')\n",
    "\n",
    "    # Chinese words segmentation using jieba\n",
    "    for i,topic in enumerate(wordlists):\n",
    "        topic = jieba.cut(topic)\n",
    "        topic_joined = ' '.join(topic)\n",
    "        topic_split = topic_joined.split()\n",
    "        wordlists[i] = topic_split\n",
    "\n",
    "    # Delete numbers\n",
    "    for i,topic in enumerate(wordlists):\n",
    "        for n, word in enumerate(topic):\n",
    "            for num in range(100):\n",
    "                word = word.replace(str(num), '')\n",
    "            for punc in punctuation:\n",
    "                word = word.replace(punc, '')\n",
    "            if word in stopwords:\n",
    "                word = ''\n",
    "            topic[n] = word\n",
    "        topic = ' '.join(topic).split()\n",
    "        wordlists[i] = topic\n",
    "        \n",
    "    return wordlists\n",
    "\n",
    "wordlists = clean_data_create_wordlists(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seg_list with all words, a vocabulary list, and a dataframe with vocabulary and frequency of ocurrance\n",
    "def create_seg_list_and_vocabulary(wordlists):\n",
    "    \n",
    "    # Create seg_list with all words in wordlists\n",
    "    seg_list = []\n",
    "    for wordlist in wordlists:\n",
    "        for word in wordlist:\n",
    "            seg_list.append(word)\n",
    "\n",
    "    # Get vocabulary\n",
    "    def get_vocabulary(seg_list):\n",
    "        voc = []\n",
    "        for word in seg_list:\n",
    "            if word not in voc: \n",
    "                voc.append(word)\n",
    "        voc = list(set(voc))\n",
    "        voc.sort()\n",
    "        return voc\n",
    "\n",
    "    # Then print the length of the vocabulary \n",
    "    vocabulary = get_vocabulary(seg_list)\n",
    "    print('The length of vocabulary is ', end=' ')\n",
    "    print(len(vocabulary))\n",
    "\n",
    "    # Create vocabulary frequency dataframe\n",
    "    vocab_freq_df = pd.DataFrame(index = range(len(vocabulary)), columns = ['vocabulary','count'])\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        vocab_freq_df.vocabulary[i] = word\n",
    "        vocab_freq_df['count'][i] = seg_list.count(word)\n",
    "\n",
    "    # Sort vocab_freq_df\n",
    "    vocab_freq_df = vocab_freq_df.sort_values('count', ascending = False)\n",
    "    \n",
    "    return seg_list, vocabulary, vocab_freq_df\n",
    "\n",
    "seg_list, vocabulary, vocab_freq_df = create_seg_list_and_vocabulary(wordlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100-word chunks\n",
    "def flatten_and_overlap(seg_list, window_size=100, overlap=25):\n",
    "    \n",
    "    # create the list of overlapping documents\n",
    "    new_list_of_documents = []\n",
    "\n",
    "    # create chunks of 100 words\n",
    "    high = window_size\n",
    "    while high < len(seg_list):\n",
    "        low = high - window_size\n",
    "        new_list_of_documents.append(seg_list[low:high])\n",
    "        high += overlap\n",
    "    return new_list_of_documents\n",
    "\n",
    "chunks = flatten_and_overlap(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe with the counts of words\n",
    "def docs_by_words_df(chunks, vocabulary):\n",
    "    vector_df = pd.DataFrame(0, index=np.arange(len(chunks)), columns=vocabulary)\n",
    "    \n",
    "    # fill out the matrix with counts\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        for word in chunk:\n",
    "            if word in vector_df.columns: \n",
    "                vector_df.loc[i,word] += 1\n",
    "            \n",
    "    return vector_df\n",
    "\n",
    "vector_df = docs_by_words_df(chunks, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that adds one to the current cell and takes its log if the value in the cell is not zero\n",
    "def one_plus_log(cell):\n",
    "    if cell != 0: \n",
    "        return 1 + math.log(cell)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function above to each cell of the table\n",
    "df_log = vector_df.applymap(one_plus_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applied the normalization\n",
    "def vector_normalizer(df_log):\n",
    "\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "\n",
    "    scaler = Normalizer()\n",
    "    df_log[df_log.columns] = scaler.fit_transform(df_log[df_log.columns])\n",
    "    \n",
    "    return df_log\n",
    "\n",
    "df_log = vector_normalizer(df_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute deviation vectors of each row\n",
    "def vector_length(u):\n",
    "    return np.sqrt(np.dot(u, u))\n",
    "\n",
    "def length_norm(u): \n",
    "    return u / vector_length(u)\n",
    "\n",
    "def transform_deviation_vectors(vector_df):\n",
    "    \n",
    "    # get the numpy matrix from the df\n",
    "    matrix = vector_df.values\n",
    "    \n",
    "    # compute the sum of the vectors\n",
    "    v_sum = np.sum(matrix, axis=0)\n",
    "    \n",
    "    # normalize this vector (find its average)\n",
    "    v_avg = length_norm(v_sum)\n",
    "    \n",
    "    # we iterate through each vector\n",
    "    for row in range(df_log.shape[0]):\n",
    "        \n",
    "        # this is one vector (row\n",
    "        v_i = matrix[row,:]\n",
    "        \n",
    "        # we subtract its component along v_average\n",
    "        scalar = np.dot(v_i,v_avg)\n",
    "        sub = v_avg * scalar\n",
    "        \n",
    "        # we replace the row by the deviation vector\n",
    "        matrix[row,:] = length_norm(v_i - sub)\n",
    "    \n",
    "    return vector_df\n",
    "\n",
    "vector_df = transform_deviation_vectors(df_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply agglomerative clustering here \n",
    "\n",
    "def apply_agglomertive(vector_df, clustersnum):\n",
    "    \n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "\n",
    "    ward = AgglomerativeClustering(n_clusters=clustersnum, linkage='ward').fit(vector_df.values)\n",
    "    label = ward.labels_\n",
    "    print(\"Number of points: \" + str(label.size))\n",
    "    \n",
    "    # compute the center of the cluster\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(vector_df.values, label)\n",
    "\n",
    "    return ward, clf\n",
    "\n",
    "ward, clf = apply_agglomertive(vector_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 10 words for each cluster centroid\n",
    "def topwords_clusters(vector_df, n_clusters, centroids, n_words=30, printed=True):   \n",
    "    # try to get the most informative words of each cluster\n",
    "    words = {}\n",
    "    vocabulary = vector_df.columns\n",
    "    for n in range(n_clusters):\n",
    "        words[n] = []\n",
    "        if printed: print('CLUSTER ' + str(n+1) + ': ', end='')\n",
    "        arr = centroids[n]\n",
    "        indices = arr.argsort()[-n_words:]\n",
    "        for i in indices:\n",
    "            if printed: print(vocabulary[i], end=', '),\n",
    "            words[n].append(vocabulary[i])\n",
    "        print('')\n",
    "    return top_words\n",
    "\n",
    "top_words = topwords_clusters(vector_df, clf.centroids_.shape[0], clf.centroids_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a after-clustered dataframe\n",
    "def create_clustering_df(vector_df, chunks, ward, wordlists, df):\n",
    "\n",
    "    #Create indices, list_of_chunks, labels, palette\n",
    "    from bokeh.palettes import Category10\n",
    "\n",
    "    indices = vector_df.index\n",
    "    list_of_chunks = []\n",
    "    for chunk in chunks:\n",
    "        chunk=' '.join(chunk)\n",
    "        list_of_chunks.append(chunk)\n",
    "    labels = ward.labels_ +1 \n",
    "    palette = []\n",
    "    for label in labels:\n",
    "        color = Category10[10][label-1]\n",
    "        palette.append(color)\n",
    "    spectopic = dict([(0,'Child general development'),\n",
    "                (1,'Early rearing and support'),\n",
    "                (2,'Newborn disease and prevention'),\n",
    "                (3,'Newborn food and nurition'),\n",
    "                (4,'Fetus growth'),\n",
    "                (5,'Child cognitive and social-emotional development'),\n",
    "                (6,'Preparation for pregnancy and childbirth'),\n",
    "                (7,'Diease and physical change during pregnancy'),\n",
    "                (8,'Newborn growth indicator'),\n",
    "                (9,'Post-childbirth nurition and food') ])\n",
    "    clustername = []\n",
    "    for label in labels:\n",
    "        clustern = spectopic[label-1]\n",
    "        clustername.append(clustern)\n",
    "\n",
    "    #Create lists of content for dataframe\n",
    "    知识标题 = []\n",
    "    一级分类 = []\n",
    "    二级分类 = []\n",
    "    三级分类 = []\n",
    "    四级分类 = []\n",
    "    知识ID = []\n",
    "    current_topic = 0\n",
    "    nextstart_topic = 0\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        title = []\n",
    "        firstc = []\n",
    "        secondc = []\n",
    "        thirdc = []\n",
    "        forthc = []\n",
    "        titleid = []\n",
    "        current_topic = nextstart_topic\n",
    "        n=0\n",
    "        while n < 100:\n",
    "            if chunk[n] in wordlists[current_topic]:\n",
    "                n+=1\n",
    "                if n == 26:\n",
    "                    nextstart_topic = current_topic\n",
    "            else:\n",
    "                title.append(df.loc[current_topic+1, '知识标题'])\n",
    "                if df.loc[current_topic+1, '一级分类'] not in firstc:\n",
    "                    firstc.append(df.loc[current_topic+1, '一级分类'])\n",
    "                if df.loc[current_topic+1, '二级分类'] not in secondc:\n",
    "                    secondc.append(df.loc[current_topic+1, '二级分类'])\n",
    "                if df.loc[current_topic+1, '三级分类'] not in thirdc:\n",
    "                    thirdc.append(df.loc[current_topic+1, '三级分类'])\n",
    "                if df.loc[current_topic+1, '四级分类'] not in forthc:\n",
    "                    forthc.append(df.loc[current_topic+1, '四级分类'])\n",
    "                titleid.append(df.loc[current_topic+1, '知识ID'])\n",
    "                current_topic += 1\n",
    "        知识标题.append(title)\n",
    "        一级分类.append(firstc)\n",
    "        二级分类.append(secondc)\n",
    "        三级分类.append(thirdc)\n",
    "        四级分类.append(forthc)\n",
    "        知识ID.append(titleid)\n",
    "\n",
    "    # Create dataframe\n",
    "    master = {'indices': indices,\n",
    "              'chunk': list_of_chunks, \n",
    "              'cluster': labels,\n",
    "              'clustername': clustername,\n",
    "              'firstca': 一级分类, \n",
    "              'secondca': 二级分类, \n",
    "              'thirdca': 三级分类, \n",
    "              'forthca': 四级分类,\n",
    "              'titleID': 知识ID,\n",
    "              'title': 知识标题, \n",
    "              'palette': palette }\n",
    "    master_df = pd.DataFrame(master)\n",
    "    master_df.head()\n",
    "\n",
    "    return master_df\n",
    "\n",
    "master_df = create_clustering_df(vector_df, chunks, ward, wordlists, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create articles count dataframe\n",
    "\n",
    "def numarticle_count_df(master_df):\n",
    "\n",
    "    countid = pd.DataFrame(index = [1,2,3,4,5,6,7,8,9,10], columns=['cluster','idcount','clustername_zh','clustername_en','topwords'])\n",
    "    clustername_zh = ['儿童总体发展','启蒙教育与辅助','新生儿疾病安全预防与','新生儿事务与营养','胎儿与新生儿生长发育',\n",
    "                      '儿童认知和社会性情感发展','备孕与生产','孕期症状和变化','新生儿成长指标','产后营养与饮食']\n",
    "    clustername_en = ['Child general development','Early rearing and support','Newborn illness and prevention',\n",
    "                      'Newborn food and nurition','Fetus growth','Child cognitive and social-emotional development',\n",
    "                      'Preparation for pregnancy and childbirth','Diease and physcial change during pregnancy','Newborn growth indicator',\n",
    "                      'Post-childbirth nurition and food']\n",
    "\n",
    "    titlelist = [[ ],[ ],[ ],[ ],[ ],[ ],[ ],[ ],[ ],[ ]]\n",
    "\n",
    "    for eachindex in master_df.index:\n",
    "        clusternum = master_df.loc[eachindex, 'cluster']\n",
    "        titleID = master_df.loc[eachindex, 'titleID']\n",
    "        for eachid in titleID:\n",
    "            if eachid not in titlelist[clusternum-1]:\n",
    "                titlelist[clusternum-1].append(eachid)\n",
    "\n",
    "    for i, clusterlist in enumerate(titlelist):\n",
    "        countid.loc[i+1,'cluster'] = i+1\n",
    "        countid.loc[i+1,'idcount'] = len(clusterlist)\n",
    "        countid.loc[i+1,'clustername_zh'] = clustername_zh[i]\n",
    "        countid.loc[i+1,'clustername_en'] = clustername_en[i]\n",
    "        countid.loc[i+1,'topwords'] = top_words[i]\n",
    "\n",
    "    return countid\n",
    "\n",
    "countid = numarticle_count_df(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visulization English\n",
    "from bokeh.plotting import ColumnDataSource, figure,show\n",
    "from bokeh.io import output_notebook, output_file, curdoc\n",
    "from bokeh.models import HoverTool, Select, Slider\n",
    "from bokeh.layouts import row, column, gridplot\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn, PreText\n",
    "\n",
    "#interactive cluster display\n",
    "source = ColumnDataSource(master_df)\n",
    "plot = figure(plot_width = 1200, tools='box_select, lasso_select, pan, box_zoom', x_axis_label = 'Chunk', y_axis_label = 'Speculative topic')\n",
    "plot.circle('indices', 'cluster', source=source, color = 'palette', hover_fill_color = 'firebrick', hover_alpha= 0.5, hover_line_color='white')\n",
    "\n",
    "hover = HoverTool(tooltips = [('Original category', '@secondca')], mode = 'vline')\n",
    "plot.add_tools(hover)\n",
    "\n",
    "#histogram\n",
    "histsource = ColumnDataSource(countid)\n",
    "histplot = figure(title='Distribution of articles', tools='', background_fill_color=\"#fafafa\", x_axis_label = 'Cluster', y_axis_label = 'Number of articles')\n",
    "histplot.hbar(y='cluster', height=0.5, left=0, right='idcount', color=\"#CAB2D6\", source =histsource)\n",
    "histhover = HoverTool(tooltips = [('Cluster','@cluster'),('Number of articles', '@idcount'),('Cluster name', '@clustername_en')], mode = 'hline')\n",
    "histplot.add_tools(histhover)\n",
    "\n",
    "#table of titles\n",
    "columns = [\n",
    "TableColumn(field=\"cluster\", title=\"Cluster\", width=80),\n",
    "TableColumn(field=\"clustername_en\", title=\"Cluster name\", width=250),\n",
    "TableColumn(field=\"idcount\", title=\"Frequency\", width=100),\n",
    "]\n",
    "data_table = DataTable(source=histsource, columns=columns,width=500)\n",
    "\n",
    "#table of topwords\n",
    "columns = [\n",
    "TableColumn(field=\"cluster\", title=\"Cluster\", width=80),\n",
    "TableColumn(field=\"clustername_en\", title=\"Cluster name\", width=250),\n",
    "TableColumn(field=\"topwords\", title=\"Topwords\", width=770),\n",
    "]\n",
    "topwords_table = DataTable(source=histsource, columns=columns,width=1100)\n",
    "\n",
    "\n",
    "#Text\n",
    "pre = PreText(text=\"\"\"Babytree titles clustering analysis\n",
    "\n",
    "\"\"\",\n",
    "width=500, height=100)\n",
    "\n",
    "\n",
    "#layout and output\n",
    "layout = gridplot([[pre,None],[topwords_table, None],[histplot,data_table],[plot,None]])\n",
    "show(layout)\n",
    "output_file('/Users/peizhiwen/Desktop/S435 Final Project/Babytree Visulization.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
