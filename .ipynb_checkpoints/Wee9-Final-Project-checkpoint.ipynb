{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**: describe your dataset, and why you're interested in it\n",
    "\n",
    "The dataset I will use for this study is a list of titles from a self-media company in China. More than ten thousand titles were retrieved online as well as the draft categories of these titles. \n",
    "I am interested in it because this company is the largest self-media source in China, mainly publishing pregnancy and early child-rearing short articles. Thus, through clustering these titles, I want to explore the most discussed topics in this area and validate the draft categories designed by the company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Research question(s)**: describe the overall research question of your project\n",
    "\n",
    "What are the most discussed topics about pregnancy and early child-rearing through clustering the titles of article?\n",
    "\n",
    "Can the clusters of titles validate the categorizations of the topics by the self-media company?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypotheses**:\n",
    "    * Describe 2-3 hypotheses that you're planning to test with your dataset\n",
    "    * Each hypoteses should be based on academic research (cite a paper) and/or background knowledge that you have about the dataset if you've collected it yourself (e.g., if you've conducted interviews)\n",
    "    * Each hypotheses should be formulated as an affirmation (and not a question)\n",
    "    * You can also describe alternative hypotheses, if you think that your results could go either way (but again, have a rationale as for why)\n",
    "    \n",
    "    The clusters can reflect the most discussed topics.\n",
    "    The clusters can validate the categorization, but more data is needed, such as the content of each article, in order to accurately validate the categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "    * how are you planning to test each hypothesis? What models are you thinking of using?\n",
    "    * what are the best results you can hope for? Is that interesting / relevant for other researchers?\n",
    "    * what are implications of your potential findings for practioners?\n",
    "    \n",
    "    K-means and DBScan will be the primary methods I will use in this study.\n",
    "    The best results may be the correct validation of the categorization. More interestingly, if the clusters can reflect the frequency of each category, I can conclude that which category occurs the most.\n",
    "    Consequently, the frequency of each category can be reported to the company so that they can write more articles that both interest parents and did not mention frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Threads**\n",
    "    * Describe issues that might arise during the analyses above\n",
    "    * Come up with backup plans in case you run into theses issues\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your raw data below; provide definition / explanations for the measures you're using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean you data in this section, and make sure it's ready to be analyzed for next week!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import jieba   #Chinese word segmentation tool\n",
    "import pandas as pd\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/peizhiwen/Desktop/S435 Final Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/peizhiwen/Desktop/S435 Final Project')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "# The data was original in excel and was exported to utf-16 txt file, so that it can be import in python\n",
    "with codecs.open('Content.txt', 'rb', encoding = 'utf-16') as f:\n",
    "    documents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the column name; replace all segementations to \\t\n",
    "documents = documents[36:]\n",
    "documents = [doc.replace('\\n', '') for doc in documents]\n",
    "documents = [doc.replace(' \"', '') for doc in documents]\n",
    "documents = [doc.replace('\"', '') for doc in documents]\n",
    "documents = [doc.replace('\\r', '\\t') for doc in documents]\n",
    "documents = ''.join(documents)\n",
    "documents = documents.split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe\n",
    "index = range(0,12050)\n",
    "df = pd.DataFrame(index = index, columns=['一级分类', '二级分类','三级分类','四级分类','五级分类','知识ID','知识标题'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace each cell of the dataframe with elements in documents\n",
    "\n",
    "a=0\n",
    "b=0\n",
    "for i, word in enumerate(documents):    \n",
    "    if documents[i] == '孕期' or documents[i] == '全龄' or documents[i] == '原创栏目' or documents[i] == '备孕' or documents[i] == '育儿':\n",
    "        b=0\n",
    "        a+=1\n",
    "        df.iloc[a,b] = documents[i]\n",
    "    else:\n",
    "        b+=1\n",
    "        df.iloc[a,b] = documents[i]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the row without data\n",
    "df = df.drop(df.index[0])\n",
    "df = df.drop(df.index[12043:])\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word list of all titles\n",
    "wordlist = []\n",
    "for i in df.index:\n",
    "    title = df.loc[i, '知识标题']\n",
    "    wordlist.append(title)\n",
    "wordlist = ' '.join(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese words segmentation using jieba\n",
    "seg_list = jieba.cut(wordlist)\n",
    "seg_list = ' '.join(seg_list)\n",
    "seg_list = seg_list.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean punctuations\n",
    "punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '−', \n",
    "               '，', '。', '！', '“', '”', '￥', \"‘\" , \"’\", '（',\n",
    "               '）', '；', '：', '——', '《', '》', '？', '【',\n",
    "               '】', '—', '……', '「', '」', '、', '＆', '＋',\n",
    "               '－', '５', '＝', '＞',  'Ｎ', '｜', '～']\n",
    "\n",
    "for i,doc in enumerate(seg_list): \n",
    "    for punc in punctuation: \n",
    "        doc = doc.replace(punc, ' ')\n",
    "    seg_list[i] = doc\n",
    "\n",
    "print(len(seg_list))\n",
    "print(' '.join(seg_list)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "for i,doc in enumerate(seg_list): \n",
    "    for num in range(10):\n",
    "        doc = doc.replace(str(num), '')\n",
    "    seg_list[i] = doc\n",
    "    \n",
    "print(len(seg_list))\n",
    "print(' '.join(seg_list)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Chinese stopwords\n",
    "with codecs.open('chinese_stopwords_list_text.txt', 'rb', encoding = 'utf-8') as f:\n",
    "    stopwords = f.read()\n",
    "stopwords = stopwords[1:]\n",
    "stopwords = stopwords.split('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace stop words with space\n",
    "for i,doc in enumerate(seg_list):\n",
    "    for stopword in stopwords:\n",
    "        doc = doc.replace(' ' + stopword + ' ', ' ')\n",
    "    seg_list[i] = doc\n",
    "\n",
    "print(len(seg_list))\n",
    "print(' '.join(seg_list)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = ' '.join(seg_list)\n",
    "seg_list = seg_list.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary\n",
    "def get_vocabulary(seg_list):\n",
    "    voc = []\n",
    "    for word in seg_list:\n",
    "        if word not in voc: \n",
    "            voc.append(word)\n",
    "    voc = list(set(voc))\n",
    "    voc.sort()\n",
    "    return voc\n",
    "\n",
    "# Then print the length of your vocabulary (it should be \n",
    "# around 5500 words)\n",
    "vocabulary = get_vocabulary(seg_list)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100-word chunks\n",
    "def flatten_and_overlap(seg_list, window_size=100, overlap=25):\n",
    "    \n",
    "    # create the list of overlapping documents\n",
    "    new_list_of_documents = []\n",
    "\n",
    "    # create chunks of 100 words\n",
    "    high = window_size\n",
    "    while high < len(seg_list):\n",
    "        low = high - window_size\n",
    "        new_list_of_documents.append(seg_list[low:high])\n",
    "        high += overlap\n",
    "    return new_list_of_documents\n",
    "\n",
    "chunks = flatten_and_overlap(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
